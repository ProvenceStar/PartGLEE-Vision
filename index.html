<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects">
  <meta name="keywords" content="Foundation Model, Hierarchical Recognition, Part Segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <!-- <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"> -->
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/eye-fill.svg">
  <!-- <link rel="stylesheet" href="./static/css/float.css"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Highlight Text Example</title>
  <style>
  .highlight {
    color:  #d35400
  }
  </style>

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Responsive Image Height with Equal Heights</title>
  <style>
    .image-container {
      display: flex;  
      flex-wrap: wrap;  
      align-items: stretch;  
    }

    .image-box {
      flex: 1; 
      margin: 5px;  
      display: flex;  
      flex-direction: column; 
    }

    .image-box img {
      width: 100%;  
      height: auto;  
      display: block; 
    }

    .image-box figcaption {
      text-align: center;  
      margin-top: auto;  
    }
  </style>


  <title>Centered YouTube Video</title>
  <style>
      .video-container {
          text-align: center;  
      }
      .video-iframe {
          margin: auto;  
          display: block; 
      }
  </style>


</head>
<body>
  
  

 
 
<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/ProvenceStar">Junyi Li</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://wjf5203.github.io/">Junfeng Wu</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://weizhi-zhao.github.io/">Weizhi Zhao</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://songbai.site/">Song Bai</a><sup>2<math xmlns="http://www.w3.org/1998/Math/MathML"></math></sup>,</span>
            <span class="author-block">
              <a href="http://vlrlab.aia.hust.edu.cn/">Xiang Bai</a><sup>1<math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€ </mo></math></sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Huazhong University of Science and Technology,</span>
            <span class="author-block"><sup>2</sup>ByteDance Inc.</span>
          </div>


          
          <div class="is-size-5 publication-authors">
            ECCV 2024
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">(<sup>*</sup>Equal Technical Contribution,<sup><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€ </mo></math></sup>Correspondence)<sup></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2407.16696"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2407.16696"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- demo Link. -->
              <!--span class="link-block">
                <a href="https://huggingface.co/spaces/Junfeng5/GLEE_demo" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="far fa-images"></i>
                  </span>
                  <span>Demo</span>
                </a>
              </span-->
              <!-- Demo Link(Faster). -->
              <!--span class="link-block">
                <a href="http://46fa6a7346679f7635.glee-vision.tech" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="far fa-images"></i>
                  </span>
                  <span>Demo(Faster)</span>
                </a>
              </span-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/FoundationVision/GLEE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(Coming Soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/data_vis.jpg"
                class="interpolation-image"
                alt="Interpolate start reference image."/>
      <h2 class="subtitle has-text-centered">
        <figcaption style="font-size: 14px;text-align: left;">
          PartGLEE is a part-level foundation model for locating and identifying both objects and their consitute parts within an image.
          Through our proposed Q-Former architecture, PartGLEE is able to perform joint-training on datasets of different granularities,
          transferring the strong generalization capability from object-level instances to their corresponding semantic parts.
          It achieves the state-of-the-art performance across various part-level tasks and obtain competitive results on object-level tasks.
         </figcaption>
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">  Abstract</h2>
        <div class="content has-text-justified">

          <p style="font-size: 20px;">
            We present PartGLEE, a part-level foundation model for locating and identifying both objects and parts in images.
            Through a unified framework, PartGLEE accomplishes detection, segmentation, and grounding of instances at any 
            granularity in the open world scenario. Specifically, we propose a Q-Former to construct the hierarchical 
            relationship between objects and parts, parsing every object into corresponding semantic parts. 
            By incorporating a large amount of object-level data, the hierarchical relationships can be extended, enabling 
            PartGLEE to recognize a rich variety of parts. We conduct comprehensive studies to validate the effectiveness of 
            our method, PartGLEE achieves the state-of-the-art performance across various part-level tasks and obtain 
            competitive results on object-level tasks. Our further analysis indicates that the hierarchical cognitive ability 
            of PartGLEE is able to facilitate a detailed comprehension in images for mLLMs.         </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">ðŸ”¥ Highlights</h2>
        <div class="content has-text-justified">

          <p style="font-size: 20px;">
            1. We construct the hierarchical relationship between objects and parts via the Q-Former, <span class="highlight">facilitating part segmentation to acquire advantages from various object-level datasets</span>.
          </p>
          <br>
          <p style="font-size: 20px;">
            2. We propose a unified pipeline for hierarchical detection and segmentation, where we <span class="highlight">first recognize objects and then parsing them into corresponding semantic parts</span>.
            This algorithm enables us to <span class="highlight">jointly detect and segment both object-level and part-level instances</span>.
          </p>
          <br>
          <p style="font-size: 20px;">
            3. We <span class="highlight">standardize the annotation granularity across various part-level datasets</span> by incorporating corresponding object-level annotations, <span class="highlight">complementing the hierarchical correspondences for current part-level datasets</span>, promoting the development of vision foundation models.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">

          <p style="font-size: 20px;">
            PartGLEE is comprised of an image encoder, a Q-Former, two independent decoders and a text encoder, as illustrated in the Figure below. We propose a Q-Former to <span class="highlight">establish the 
            hierarchical relationship between objects and parts</span>. A set of parsing queries are initialized in the Q-Former to interact with each object query, thus <span class="highlight">parsing objects into 
            their corresponding parts</span>. Our proposed Q-Former functions as a decomposer, extracting and representing parts from object queries. Hence, by training jointly on extensive object-level
            datasets and limited hierarchical datasets which contain object-part correspondences, our Q-Former obtains <span class="highlight">strong generalization ability</span> to parse any novel object into its consitute parts.
          </p>
          
        <div class="column is-centered has-text-centered">
          <img src="./static/images/pipeline.png"
                class="interpolation-image"
                alt="Interpolate start reference image."/>
                <figcaption style="font-size: 14px;text-align: left;">Framework of PartGLEE. The Q-Former takes each object query as input
                  and output the corresponding part queries. These queries are then fed into the object decoder and the part decoder 
                  respectively to generate hierarchical predictions. </figcaption>
        </div>

        </div>
      </div>
    </div>

  </div>
</section>



 

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Evaluation</h2>
        <div class="content has-text-justified">

          <p style="font-size: 20px;">
          <b>Object-level and Part-level tasks.</b> To endow our model with robust generalization capability, we perform joint
          training on various datasets and evaluate its performance on both object-level and part-level tasks. We compare our 
          model with specialist and generalist models to evaluate its performance on object-level data. Additionally, we contrast
          it with VLPart to assess its performance on part-level datasets as well as the effectiveness of joint-training process 
          on both types of datasets. PartGLEE significantly outperforms VLPart on both object-level and part-level tasks after
          joint-training, while achieving comparable performance on object-level tasks compared with previous SOTA.
          </p>

      <div class="image-container">
        <div class="image-box">
          <img src="./static/images/object-part-tasks.png" alt="Image 1">
          <figcaption>Joint-Training Performance of PartGLEE.</figcaption>
        </div>
      </div>

      <br><br>


      <p style="font-size: 20px;">
        <b>Performance on Traditional Object-Level Tasks.</b> To illustrate the versatility and effectiveness of our model, we further compare the performance of our model with recent 
          specialist and generalist models on traditional object-level tasks. It turns out that our model achieves state-of-the-art 
          performance on part-level tasks, while maintaining competitive performance on object-level tasks. </p>

        <div class="image-container">
          <div class="image-box">
            <img src="./static/images/traditional-image-tasks.png" alt="Image 2">
            <figcaption>Comparison between PartGLEE with recent specialist and generalist models on traditional object-level tasks.</figcaption>
          </div>
        </div>

      <br>

        </div>
      </div>
    </div>

  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Visualization</h2>
        <div class="content has-text-justified">

      <div class="image-container">
        <div class="image-box">
          <img src="./static/images/SAM-Comparison.png" alt="Image 3">
          <figcaption>Comparison of visualization results between SAM and PartGLEE.</figcaption>
        </div>
      </div>

      <br><br>

      <div class="image-container">
        <div class="image-box">
          <img src="./static/images/Generalization_Visualization.jpg" alt="Image 4">
          <figcaption>Visualization of the generalization capability of PartGLEE.</figcaption>
        </div>
      </div>

      <br><br>

      <div class="image-container">
        <div class="image-box">
          <img src="./static/images/hierarchical_data_vis.jpg" alt="Image 5">
          <figcaption>Visualization of hierarchical correspondences in part-level datasets.</figcaption>
        </div>
      </div>

      <br><br>

      <div class="image-container">
        <div class="image-box">
          <img src="./static/images/Hierarchical_SA_1B_v3.jpg" alt="Image 6">
          <figcaption> Visualization of our proposed hierarchical SA-1B.</figcaption>
        </div>
      </div>
      
      <br>

      </div>
    </div>
  </div>
</div>
</section>



<!--section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{wu2023GLEE,
  author= {Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, Song Bai},
  title = {General Object Foundation Model for Images and Videos at Scale},
  year={2023},
  eprint={2312.09158},
  archivePrefix={arXiv}
}</code></pre>
  </div>
</section-->


<footer class="footer">
  <div class="container">

     <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website is adapted from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>. 
          </p>
        </div>
      </div>
  </div>
</footer>

</body>
</html>
