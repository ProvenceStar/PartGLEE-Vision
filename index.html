<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects">
  <meta name="keywords" content="Foundation Model, Hierarchical Recognition, Part Segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <!-- <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"> -->
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/eye-fill.svg">
  <!-- <link rel="stylesheet" href="./static/css/float.css"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Highlight Text Example</title>
  <style>
  .highlight {
    color:  #d35400
  }
  </style>

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Responsive Image Height with Equal Heights</title>
  <style>
    .image-container {
      display: flex;  
      flex-wrap: wrap;  
      align-items: stretch;  
    }

    .image-box {
      flex: 1; 
      margin: 5px;  
      display: flex;  
      flex-direction: column; 
    }

    .image-box img {
      width: 100%;  
      height: auto;  
      display: block; 
    }

    .image-box figcaption {
      text-align: center;  
      margin-top: auto;  
    }
  </style>


  <title>Centered YouTube Video</title>
  <style>
      .video-container {
          text-align: center;  
      }
      .video-iframe {
          margin: auto;  
          display: block; 
      }
  </style>


</head>
<body>
  
  

 
 
<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Junyi Li<sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://wjf5203.github.io/">Junfeng Wu</a><sup>1*</sup>,</span>
            <span class="author-block">
              Weizhi Zhao<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://songbai.site/">Song Bai</a><sup>2<math xmlns="http://www.w3.org/1998/Math/MathML"></math></sup>,</span>
            <span class="author-block">
              <a href="http://vlrlab.aia.hust.edu.cn/">Xiang Bai</a><sup>1<math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€ </mo></math></sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Huazhong University of Science and Technology,</span>
            <span class="author-block"><sup>2</sup>ByteDance Inc.</span>
          </div>


          
          <div class="is-size-5 publication-authors">
            ECCV 2024
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">(<sup>*</sup>Equal Technical Contribution,<sup><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>â€ </mo></math></sup>Correspondence)<sup></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2312.09158.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.09158"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- demo Link. -->
              <!--span class="link-block">
                <a href="https://huggingface.co/spaces/Junfeng5/GLEE_demo" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="far fa-images"></i>
                  </span>
                  <span>Demo</span>
                </a>
              </span-->
              <span class="link-block">
                <a href="http://46fa6a7346679f7635.glee-vision.tech" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="far fa-images"></i>
                  </span>
                  <span>Demo(Faster)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/FoundationVision/GLEE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/data_vis.png"
                class="interpolation-image"
                alt="Interpolate start reference image."/>
      <h2 class="subtitle has-text-centered">
        <figcaption style="font-size: 14px;text-align: left;">
          PartGLEE is a part-level foundation model for locating and identifying both objects and their consitute parts within an image.
          Through our proposed Q-Former architecture, PartGLEE is able to perform joint-training on datasets of different granularities,
          transferring the strong generalization capability from object-level instances to their corresponding semantic parts.
          It achieves the state-of-the-art performance across various part-level tasks and obtain competitive results on object-level tasks.
         </figcaption>
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">  Abstract</h2>
        <div class="content has-text-justified">

          <p style="font-size: 20px;">
            We present PartGLEE, a part-level foundation model for locating and identifying both objects and parts in images.
            Through a unified framework, PartGLEE accomplishes detection, segmentation, and grounding of instances at any 
            granularity in the open world scenario. Specifically, we propose a Q-Former to construct the hierarchical 
            relationship between objects and parts, parsing every object into corresponding semantic parts. 
            By incorporating a large amount of object-level data, the hierarchical relationships can be extended, enabling 
            PartGLEE to recognize a rich variety of parts. We conduct comprehensive studies to validate the effectiveness of 
            our method, PartGLEE achieves the state-of-the-art performance across various part-level tasks and obtain 
            competitive results on object-level tasks. Our further analysis indicates that the hierarchical cognitive ability 
            of PartGLEE is able to facilitate a detailed comprehension in images for mLLMs.         </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">ðŸ”¥ Highlights</h2>
        <div class="content has-text-justified">

          <p style="font-size: 20px;">
            1. We construct the hierarchical relationship between objects and parts via the Q-Former, <span class="highlight">facilitating part segmentation to acquire advantages from various object-level datasets</span>.
          </p>
          <br>
          <p style="font-size: 20px;">
            2. We propose a unified pipeline for hierarchical detection and segmentation, where we <span class="highlight">first recognize objects and then parsing them into corresponding semantic parts</span>.
            This algorithm enables us to <span class="highlight">jointly detect and segment both object-level and part-level instances</span>.
          </p>
          <br>
          <p style="font-size: 20px;">
            3. We <span class="highlight">standardize the annotation granularity across various part-level datasets</span> by incorporating corresponding object-level annotations, <span class="highlight">complementing the hierarchical correspondences for current part-level datasets</span>, promoting the development of vision foundation models.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">

          <p style="font-size: 20px;">
            The proposed GLEE consists of an image encoder, a text encoder, a visual prompter, and an object decoder, as illustrated in Figure. 
            The text encoder processes arbitrary descriptions related to the task, including object categories,
            names in any form, captions about objects, and referring expressions. The visual prompter encodes user inputs such as points, 
            bounding boxes, or scribbles during interactive segmentation into corresponding visual representations of target objects. 
            Then they are integrated into a detector for extracting objects from images according to textual and visual input.          
          </p>
          
        <div class="column is-centered has-text-centered">
          <img src="./static/images/pipeline.png"
                class="interpolation-image"
                alt="Interpolate start reference image."/>
                <figcaption style="font-size: 14px;text-align: left;">Framework of GLEE. The text encoder accepts textual descriptions in various forms from diverse data sources, including
                  object categories, names, captions, and referring expressions. The visual prompter encodes points, bounding boxes, or scribbles into
                  corresponding visual representations.The object decoder take them and image features to predict objects in images. </figcaption>
        </div>


        <p style="font-size: 20px;">
        Based on the above designs, GLEE can be used to seamlessly unify a wide range of object perception tasks in images and videos, 
        including object detection, instance segmentation, grounding, multi-target tracking (MOT), video instance segmentation (VIS), 
        video object segmentation (VOS), interactive segmentation and tracking, and <span class="highlight">supports open-world/large-vocabulary image and video detection and segmentation tasks</span>.
        A visual foundation model should be able to easily scale up the training data and achieve better generalization performance. 
        Thanks to the unified training paradigm, the training data of GLEE can be scaled up at low cost
        by introducing a large amount of automatically labeled data.
        </p>


        </div>
      </div>
    </div>

  </div>
</section>



 

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Evaluation</h2>
        <div class="content has-text-justified">

          <p style="font-size: 20px;">
          <b>Image tasks.</b> We demonstrate the universality and effectiveness of GLEE 
          model as an object-level visual foundation model, directly
          applicable to various object-centric tasks while ensuring
          state-of-the-art performance without needing fine-tuning.
          </p>
          <!-- <p style="font-size: 20px;">
          <b>Zero-shot Transfer to Video Tasks. </b> The proposed GLEE
          is capable of adapting to new data and even new tasks in
          a zero-shot manner, without the need for additional finetuning. We evaluate its zero-shot capability on three largescale, large-vocabulary open-world video tracking datasets:
          TAO, BURST, and LV-VIS. <span class="highlight">Notably, the GLEE here is from
          the image-level joint training, which has not been exposed to
          images from these three datasets nor trained on videolevel data.</span> Despite these constraints, GLEE achieves stateof-the-art performance that significantly exceeds existing
          methodologies. 
          </p> -->


     

      <div class="image-container">
        <div class="image-box">
          <img src="./static/images/imagetask.png" alt="Image 1">
          <figcaption>Comparison of GLEE to recent specialist and generalist models on object-level image tasks.</figcaption>
        </div>
        <!-- <div class="image-box">
          <img src="./static/images/videotask.png" alt="Image 2">
          <figcaption>Comparison of GLEE to recent specialist and generalist models on object-level video tasks in a zero-shot manner.</figcaption>
        </div> -->
      </div>

      <br><br>


      <p style="font-size: 20px;">
        <b>Data Scale.</b> Train GLEE-Pro with 10%,
        20%, 50%, 100% of the training data to evaluate the performance on zero-shot transfer tasks, including TAO, BURST,
        OVIS, and YTVIS. Increased sizes of training
        datasets result in enhanced zero-shot performance across diverse downstream tasks. </p>
      <p style="font-size: 20px;">  
       <b>Serve as Foundation Model.</b> 
       We substituted LISA vision backbone with a frozen, pretrained GLEE-Plus and fed the object queries from GLEE into LLAVA and remove decoder of
    LISA. We directly dot product the output SEG tokens with
    GLEE feature map to generate masks. After training for the same number of steps, our modified LISA-GLEE achieved comparable results to the original version, demonstrating the versatility of representations
    from GLEE and its <span class="highlight">effectiveness in serving other models</span>.

      </p>
      <br>


      <div class="image-container">
        <div class="image-box">
          <img src="./static/images/scaleup_performance.png" alt="Image 1">
          <figcaption>Data scaling. The performance of GLEE-Pro after training on 10%, 20%, 50%, 100%.</figcaption>
        </div>
        <div class="image-box">
          <img src="./static/images/LISA.png" alt="Image 2">
          <figcaption>The performance comparison of replacing SAM with
GLEE in LISA, GLEE achieves the same effectiveness as SAM in
extracting objects.</figcaption>
        </div>
      </div>



        </div>
      </div>
    </div>

  </div>
</section>



 



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{wu2023GLEE,
  author= {Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, Song Bai},
  title = {General Object Foundation Model for Images and Videos at Scale},
  year={2023},
  eprint={2312.09158},
  archivePrefix={arXiv}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">

     <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website is adapted from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
